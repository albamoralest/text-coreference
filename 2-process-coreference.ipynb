{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3591d7a5-1f93-4105-ba6e-9c2b56d16368",
   "metadata": {},
   "source": [
    "# Processing coreference using a set of PERIODICAL's biographies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0f1da7-d6c2-4c8a-987f-0e87cd6ccf10",
   "metadata": {},
   "source": [
    "Requirements\n",
    "\n",
    "Citation\n",
    "    spacy library\n",
    "    coref library: \"fastcoref\"\n",
    "    Repository: https://github.com/shon-otmazgin/fastcoref#demo\n",
    "    \n",
    "    @inproceedings{Otmazgin2022FcorefFA,\n",
    "      title={F-coref: Fast, Accurate and Easy to Use Coreference Resolution},\n",
    "      author={Shon Otmazgin and Arie Cattan and Yoav Goldberg},\n",
    "      booktitle={AACL},\n",
    "      year={2022}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99a315ca-6b7f-4f23-893e-4a4cd5a922f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/user-data/amt557/meetups_env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "01/12/2024 16:52:46 - INFO - \t missing_keys: []\n",
      "01/12/2024 16:52:46 - INFO - \t unexpected_keys: []\n",
      "01/12/2024 16:52:46 - INFO - \t mismatched_keys: []\n",
      "01/12/2024 16:52:46 - INFO - \t error_msgs: []\n",
      "01/12/2024 16:52:46 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['transformer', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner', 'entityfishing', 'fastcoref']\n"
     ]
    }
   ],
   "source": [
    "from fastcoref import spacy_component\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "# smaller dataset \n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# for entity recognition\n",
    "# entity linking\n",
    "import spacyfishing\n",
    "# specify configuration:\n",
    "nlp.add_pipe(\"entityfishing\", config={\"extra_info\": True})\n",
    "\n",
    "# add to the pipeline fastcoref\n",
    "import spacy_transformers\n",
    "from spacy.tokens import Doc\n",
    "nlp.add_pipe(\"fastcoref\")\n",
    "\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e55d3668-da0b-4d10-8411-114e5ee9e4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import glob\n",
    "# to eliminate accents\n",
    "# from unidecode import unidecode\n",
    "# to clear memory\n",
    "import gc\n",
    "pd.options.mode.copy_on_write = True\n",
    "# time execution\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fee4196-acd7-4d1d-9c5a-8b9d4f5d0fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40 entries, 0 to 39\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   qid         40 non-null     object\n",
      " 1   dbpedia_id  40 non-null     object\n",
      " 2   resource    40 non-null     object\n",
      "dtypes: object(3)\n",
      "memory usage: 1.1+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>dbpedia_id</th>\n",
       "      <th>resource</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q254</td>\n",
       "      <td>33163</td>\n",
       "      <td>http://dbpedia.org/resource/Wolfgang_Amadeus_M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q130759</td>\n",
       "      <td>99636</td>\n",
       "      <td>http://dbpedia.org/resource/Christoph_Willibal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q364020</td>\n",
       "      <td>376771</td>\n",
       "      <td>http://dbpedia.org/resource/Étienne_Méhul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q168485</td>\n",
       "      <td>261539</td>\n",
       "      <td>http://dbpedia.org/resource/Gaspare_Spontini</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q154203</td>\n",
       "      <td>354604</td>\n",
       "      <td>http://dbpedia.org/resource/Albert_Lortzing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       qid dbpedia_id                                           resource\n",
       "0     Q254      33163  http://dbpedia.org/resource/Wolfgang_Amadeus_M...\n",
       "1  Q130759      99636  http://dbpedia.org/resource/Christoph_Willibal...\n",
       "2  Q364020     376771          http://dbpedia.org/resource/Étienne_Méhul\n",
       "3  Q168485     261539       http://dbpedia.org/resource/Gaspare_Spontini\n",
       "4  Q154203     354604        http://dbpedia.org/resource/Albert_Lortzing"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# File with list of biographies to process\n",
    "# headers = ['qid', 'dbpedia_id', 'resource']\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('matching_bios.csv')\n",
    "df['dbpedia_id'] = df['dbpedia_id'].astype(str)\n",
    "df = df.drop_duplicates(subset=['dbpedia_id'])\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73df849d-6dde-4a16-87bf-bfd9ae8d6830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List files available\n",
    "def find_files(folder_path, file_pattern):\n",
    "    \"\"\"\n",
    "    Find files in a folder based on a specified pattern.\n",
    "\n",
    "    Parameters:\n",
    "    - folder_path: The path to the folder.\n",
    "    - file_pattern: The pattern to match for file names (e.g., '*.txt').\n",
    "\n",
    "    Returns:\n",
    "    - A list of file paths that match the specified pattern.\n",
    "    \"\"\"\n",
    "    search_pattern = f\"{folder_path}/{file_pattern}\"\n",
    "    files = glob.glob(search_pattern)\n",
    "    # next line list only the file names, if commented then the function\n",
    "    # retunrs the entire path\n",
    "    # file_names = [os.path.basename(file) for file in files]\n",
    "    return files\n",
    "\n",
    "# Example usage:\n",
    "# folder_path = '../meetups_pilot/cacheCoref-fastcoref'\n",
    "# file_pattern = '31281773_*.coref'\n",
    "\n",
    "# found_files = find_files(folder_path, file_pattern)\n",
    "\n",
    "# if found_files:\n",
    "#     print(\"Found files:\")\n",
    "#     for file_path in found_files:\n",
    "#         print(file_path)\n",
    "# else:\n",
    "#     print(\"No files found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0856eea-9a29-47ef-8d34-31cf7692f035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "match = re.search(r'_(\\d+)\\.coref', '../meetups_pilot/cacheCoref-fastcoref/31281773_9.coref')\n",
    "print(int(match.group(1)) if match else None)\n",
    "\n",
    "paragraph_number = int(re.search(r'_(\\d+)\\.coref', file_path).group(1)) if re.search(r'_(\\d+)\\.coref', file_path) else None\n",
    "print(paragraph_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9026160-9363-4c15-9659-3719d1688ba3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found files... 33163\n",
      "../meetups_pilot/cacheCoref-fastcoref/33163_0.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/33163_1.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/33163_4.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/33163_5.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/33163_6.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/33163_7.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/33163_8.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/33163_9.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/33163_10.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/33163_11.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/33163_12.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/33163_13.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/33163_14.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/33163_15.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/33163_16.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/33163_18.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/33163_19.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/33163_20.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/33163_21.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/33163_22.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/33163_23.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/33163_25.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/33163_26.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/33163_27.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/33163_28.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/33163_29.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/33163_30.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/33163_31.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/33163_32.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/33163_33.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/33163_34.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/33163_35.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/33163_37.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/33163_38.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/33163_39.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/33163_42.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/33163_43.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/33163_45.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/33163_46.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/33163_47.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/33163_48.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/33163_49.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/33163_50.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/33163_51.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/33163_52.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/33163_53.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/33163_54.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/33163_56.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/33163_57.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/33163_58.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/33163_59.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/33163_60.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/33163_61.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/33163_63.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/33163_65.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/33163_67.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/33163_68.coref\n",
      "Found files... 99636\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_0.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_1.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_3.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_4.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_5.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_6.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_7.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_8.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_9.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_10.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_12.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_13.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_14.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_15.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_16.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_17.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_19.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_20.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_21.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_22.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_24.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_25.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_26.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_27.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_28.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_30.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_32.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_33.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_34.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_35.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_36.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_37.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_38.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_40.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_41.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_42.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_44.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_46.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_50.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_51.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_52.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_53.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_54.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_55.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_56.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_57.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_58.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_59.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_60.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_61.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_62.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_63.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_64.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_65.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_66.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_67.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_68.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_69.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_70.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_71.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_73.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_74.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_75.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_76.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_77.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_78.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_79.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_80.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_81.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_82.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_83.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_84.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_85.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_86.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_87.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_88.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_90.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_91.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_92.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/99636_93.coref\n",
      "Found files... 376771\n",
      "../meetups_pilot/cacheCoref-fastcoref/376771_0.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/376771_2.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/376771_3.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/376771_4.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/376771_5.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/376771_6.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/376771_9.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/376771_10.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/376771_11.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/376771_12.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/376771_13.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/376771_14.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/376771_15.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/376771_16.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/376771_20.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/376771_21.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/376771_23.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/376771_24.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/376771_25.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/376771_26.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/376771_27.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/376771_28.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/376771_29.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/376771_30.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/376771_33.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/376771_34.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/376771_35.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/376771_36.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/376771_38.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/376771_40.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/376771_41.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/376771_42.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/376771_43.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/376771_45.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/376771_46.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/376771_47.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/376771_48.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/376771_49.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/376771_50.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/376771_51.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/376771_52.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/376771_53.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/376771_54.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/376771_55.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/376771_56.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/376771_57.coref\n",
      "Found files... 261539\n",
      "../meetups_pilot/cacheCoref-fastcoref/261539_0.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/261539_2.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/261539_3.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/261539_4.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/261539_5.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/261539_6.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/261539_7.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/261539_10.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/261539_11.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/261539_12.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/261539_13.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/261539_14.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/261539_16.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/261539_20.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/261539_21.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/261539_22.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/261539_23.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/261539_24.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/261539_25.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/261539_26.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/261539_28.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/261539_30.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/261539_31.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/261539_32.coref\n",
      "Found files... 354604\n",
      "../meetups_pilot/cacheCoref-fastcoref/354604_0.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/354604_2.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/354604_3.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/354604_4.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/354604_5.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/354604_6.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/354604_7.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/354604_8.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/354604_9.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/354604_10.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/354604_12.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/354604_13.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/354604_14.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/354604_15.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/354604_16.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/354604_17.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/354604_18.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/354604_19.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/354604_20.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/354604_21.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/354604_22.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/354604_23.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/354604_24.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/354604_25.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/354604_26.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/354604_28.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/354604_30.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/354604_31.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/354604_32.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/354604_33.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/354604_35.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/354604_36.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/354604_37.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/354604_38.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/354604_39.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/354604_40.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/354604_41.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/354604_44.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/354604_47.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/354604_48.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/354604_49.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/354604_50.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/354604_51.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/354604_52.coref\n",
      "Found files... 9514\n",
      "../meetups_pilot/cacheCoref-fastcoref/9514_33.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/9514_29.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/9514_30.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/9514_31.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/9514_32.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/9514_26.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/9514_27.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/9514_28.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/9514_22.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/9514_23.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/9514_24.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/9514_25.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/9514_21.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/9514_17.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/9514_18.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/9514_19.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/9514_20.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/9514_13.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/9514_14.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/9514_15.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/9514_16.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/9514_10.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/9514_12.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/9514_9.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/9514_8.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/9514_6.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/9514_5.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/9514_4.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/9514_2.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/9514_0.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/9514_34.coref\n",
      "Found files... 2232700\n",
      "../meetups_pilot/cacheCoref-fastcoref/2232700_0.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2232700_2.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2232700_3.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2232700_5.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2232700_6.coref\n",
      "Found files... 16816904\n",
      "../meetups_pilot/cacheCoref-fastcoref/16816904_0.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/16816904_1.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/16816904_2.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/16816904_4.coref\n",
      "Found files... 214042\n",
      "../meetups_pilot/cacheCoref-fastcoref/214042_0.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214042_1.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214042_2.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214042_5.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214042_6.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214042_8.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214042_9.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214042_10.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214042_12.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214042_14.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214042_16.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214042_17.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214042_18.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214042_20.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214042_21.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214042_22.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214042_23.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214042_24.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214042_25.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214042_26.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214042_27.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214042_28.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214042_30.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214042_32.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214042_34.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214042_36.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214042_38.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214042_39.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214042_41.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214042_43.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214042_45.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214042_47.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214042_48.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214042_49.coref\n",
      "Found files... 324889\n",
      "../meetups_pilot/cacheCoref-fastcoref/324889_0.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/324889_2.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/324889_3.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/324889_5.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/324889_6.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/324889_7.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/324889_10.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/324889_11.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/324889_12.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/324889_13.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/324889_14.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/324889_15.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/324889_16.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/324889_17.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/324889_18.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/324889_19.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/324889_21.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/324889_22.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/324889_23.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/324889_24.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/324889_25.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/324889_26.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/324889_27.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/324889_29.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/324889_30.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/324889_31.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/324889_32.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/324889_33.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/324889_34.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/324889_35.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/324889_36.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/324889_37.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/324889_38.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/324889_40.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/324889_41.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/324889_42.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/324889_43.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/324889_44.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/324889_45.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/324889_46.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/324889_47.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/324889_48.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/324889_49.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/324889_50.coref\n",
      "Found files... 26367137\n",
      "../meetups_pilot/cacheCoref-fastcoref/26367137_0.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/26367137_2.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/26367137_3.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/26367137_4.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/26367137_5.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/26367137_6.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/26367137_7.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/26367137_8.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/26367137_9.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/26367137_11.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/26367137_13.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/26367137_14.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/26367137_15.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/26367137_16.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/26367137_18.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/26367137_19.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/26367137_20.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/26367137_22.coref\n",
      "Found files... 1141589\n",
      "../meetups_pilot/cacheCoref-fastcoref/1141589_0.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1141589_3.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1141589_4.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1141589_5.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1141589_6.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1141589_7.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1141589_8.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1141589_9.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1141589_10.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1141589_11.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1141589_12.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1141589_13.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1141589_14.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1141589_15.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1141589_16.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1141589_17.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1141589_19.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1141589_21.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1141589_22.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1141589_23.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1141589_24.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1141589_25.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1141589_26.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1141589_27.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1141589_28.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1141589_30.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1141589_31.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1141589_32.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1141589_33.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1141589_34.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1141589_35.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1141589_36.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1141589_37.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1141589_38.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1141589_39.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1141589_40.coref\n",
      "Found files... 634561\n",
      "../meetups_pilot/cacheCoref-fastcoref/634561_0.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/634561_1.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/634561_3.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/634561_4.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/634561_6.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/634561_7.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/634561_9.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/634561_10.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/634561_11.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/634561_13.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/634561_14.coref\n",
      "Found files... 531849\n",
      "../meetups_pilot/cacheCoref-fastcoref/531849_0.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/531849_3.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/531849_4.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/531849_5.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/531849_7.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/531849_8.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/531849_10.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/531849_11.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/531849_12.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/531849_13.coref\n",
      "Found files... 312168\n",
      "../meetups_pilot/cacheCoref-fastcoref/312168_0.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/312168_1.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/312168_3.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/312168_4.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/312168_6.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/312168_7.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/312168_8.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/312168_9.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/312168_10.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/312168_12.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/312168_13.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/312168_15.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/312168_16.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/312168_17.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/312168_19.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/312168_20.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/312168_21.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/312168_22.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/312168_23.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/312168_24.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/312168_25.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/312168_26.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/312168_27.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/312168_29.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/312168_31.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/312168_33.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/312168_34.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/312168_37.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/312168_38.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/312168_39.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/312168_41.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/312168_42.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/312168_44.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/312168_45.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/312168_49.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/312168_51.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/312168_54.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/312168_55.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/312168_56.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/312168_57.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/312168_58.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/312168_60.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/312168_61.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/312168_62.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/312168_63.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/312168_65.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/312168_66.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/312168_67.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/312168_68.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/312168_69.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/312168_70.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/312168_71.coref\n",
      "Found files... 192711\n",
      "../meetups_pilot/cacheCoref-fastcoref/192711_0.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/192711_2.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/192711_3.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/192711_5.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/192711_6.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/192711_8.coref\n",
      "Found files... 313380\n",
      "../meetups_pilot/cacheCoref-fastcoref/313380_0.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/313380_1.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/313380_2.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/313380_3.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/313380_6.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/313380_7.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/313380_8.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/313380_9.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/313380_10.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/313380_11.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/313380_12.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/313380_13.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/313380_14.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/313380_15.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/313380_16.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/313380_17.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/313380_18.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/313380_19.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/313380_20.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/313380_21.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/313380_23.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/313380_26.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/313380_27.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/313380_28.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/313380_29.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/313380_30.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/313380_31.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/313380_33.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/313380_34.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/313380_35.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/313380_37.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/313380_38.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/313380_39.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/313380_40.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/313380_41.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/313380_43.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/313380_45.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/313380_46.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/313380_47.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/313380_48.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/313380_53.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/313380_54.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/313380_55.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/313380_56.coref\n",
      "Found files... 711408\n",
      "../meetups_pilot/cacheCoref-fastcoref/711408_0.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/711408_1.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/711408_2.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/711408_3.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/711408_4.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/711408_5.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/711408_7.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/711408_9.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/711408_10.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/711408_11.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/711408_12.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/711408_13.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/711408_14.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/711408_16.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/711408_17.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/711408_18.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/711408_20.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/711408_22.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/711408_23.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/711408_24.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/711408_25.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/711408_26.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/711408_27.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/711408_28.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/711408_29.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/711408_30.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/711408_31.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/711408_32.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/711408_33.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/711408_34.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/711408_35.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/711408_36.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/711408_37.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/711408_38.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/711408_39.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/711408_40.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/711408_41.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/711408_42.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/711408_43.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/711408_44.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/711408_45.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/711408_46.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/711408_47.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/711408_48.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/711408_49.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/711408_50.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/711408_51.coref\n",
      "Found files... 28964451\n",
      "../meetups_pilot/cacheCoref-fastcoref/28964451_0.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/28964451_2.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/28964451_3.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/28964451_4.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/28964451_6.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/28964451_8.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/28964451_9.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/28964451_10.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/28964451_11.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/28964451_12.coref\n",
      "Found files... 194903\n",
      "../meetups_pilot/cacheCoref-fastcoref/194903_0.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/194903_2.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/194903_3.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/194903_4.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/194903_6.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/194903_7.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/194903_8.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/194903_9.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/194903_10.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/194903_11.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/194903_12.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/194903_13.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/194903_14.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/194903_15.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/194903_16.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/194903_17.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/194903_18.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/194903_19.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/194903_21.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/194903_22.coref\n",
      "No files found.\n",
      "Found files... 2352782\n",
      "../meetups_pilot/cacheCoref-fastcoref/2352782_0.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2352782_2.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2352782_3.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2352782_4.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2352782_5.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2352782_6.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2352782_7.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2352782_9.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2352782_10.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2352782_11.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2352782_12.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2352782_13.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2352782_14.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2352782_17.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2352782_18.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2352782_19.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2352782_20.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2352782_21.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2352782_22.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2352782_23.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2352782_24.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2352782_25.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2352782_26.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2352782_27.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2352782_28.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2352782_29.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2352782_31.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2352782_32.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2352782_34.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2352782_35.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2352782_36.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2352782_37.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2352782_38.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2352782_39.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2352782_40.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2352782_41.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2352782_43.coref\n",
      "No files found.\n",
      "Found files... 530719\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_0.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_2.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_4.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_5.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_6.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_7.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_8.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_9.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_10.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_11.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_12.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_15.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_16.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_17.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_18.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_19.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_21.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_22.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_23.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_24.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_26.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_27.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_28.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_29.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_30.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_31.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_32.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_33.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_34.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_35.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_36.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_37.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_38.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_39.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_40.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_41.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_42.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_43.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_44.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_45.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_47.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_48.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_49.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_51.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_52.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_53.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_54.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_55.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_56.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_57.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_58.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_59.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_60.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_62.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_63.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_64.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_65.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_66.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_67.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_68.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_69.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_70.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/530719_71.coref\n",
      "Found files... 43697436\n",
      "../meetups_pilot/cacheCoref-fastcoref/43697436_0.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/43697436_2.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/43697436_3.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/43697436_5.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/43697436_6.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/43697436_7.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/43697436_8.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/43697436_10.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/43697436_11.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/43697436_12.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/43697436_13.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/43697436_15.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/43697436_16.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/43697436_17.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/43697436_18.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/43697436_19.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/43697436_20.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/43697436_21.coref\n",
      "No files found.\n",
      "Found files... 600697\n",
      "../meetups_pilot/cacheCoref-fastcoref/600697_0.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/600697_1.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/600697_3.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/600697_4.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/600697_5.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/600697_6.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/600697_8.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/600697_9.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/600697_10.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/600697_11.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/600697_12.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/600697_14.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/600697_15.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/600697_17.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/600697_19.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/600697_20.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/600697_21.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/600697_22.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/600697_23.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/600697_24.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/600697_25.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/600697_26.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/600697_27.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/600697_28.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/600697_29.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/600697_30.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/600697_31.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/600697_32.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/600697_33.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/600697_34.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/600697_35.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/600697_36.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/600697_37.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/600697_38.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/600697_39.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/600697_40.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/600697_41.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/600697_42.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/600697_43.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/600697_44.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/600697_45.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/600697_46.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/600697_47.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/600697_48.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/600697_50.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/600697_51.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/600697_52.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/600697_53.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/600697_54.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/600697_56.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/600697_57.coref\n",
      "Found files... 472353\n",
      "../meetups_pilot/cacheCoref-fastcoref/472353_0.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/472353_1.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/472353_2.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/472353_5.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/472353_6.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/472353_7.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/472353_8.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/472353_10.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/472353_12.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/472353_13.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/472353_14.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/472353_16.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/472353_18.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/472353_19.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/472353_20.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/472353_24.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/472353_25.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/472353_26.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/472353_27.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/472353_28.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/472353_29.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/472353_30.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/472353_31.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/472353_32.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/472353_34.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/472353_35.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/472353_36.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/472353_37.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/472353_38.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/472353_39.coref\n",
      "Found files... 15277235\n",
      "../meetups_pilot/cacheCoref-fastcoref/15277235_0.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/15277235_2.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/15277235_3.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/15277235_4.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/15277235_5.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/15277235_6.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/15277235_7.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/15277235_8.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/15277235_9.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/15277235_11.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/15277235_12.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/15277235_13.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/15277235_14.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/15277235_15.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/15277235_17.coref\n",
      "No files found.\n",
      "Found files... 1272951\n",
      "../meetups_pilot/cacheCoref-fastcoref/1272951_0.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1272951_2.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1272951_3.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1272951_4.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1272951_6.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1272951_7.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1272951_8.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1272951_9.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1272951_10.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1272951_11.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1272951_12.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1272951_13.coref\n",
      "Found files... 214016\n",
      "../meetups_pilot/cacheCoref-fastcoref/214016_0.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214016_1.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214016_2.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214016_3.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214016_4.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214016_5.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214016_6.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214016_8.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214016_9.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214016_10.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214016_11.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214016_12.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214016_13.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214016_14.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214016_15.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214016_16.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214016_17.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214016_19.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214016_20.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214016_21.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214016_22.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214016_23.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214016_24.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214016_25.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214016_26.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214016_27.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214016_28.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214016_29.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214016_31.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214016_32.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214016_33.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/214016_34.coref\n",
      "Found files... 9984016\n",
      "../meetups_pilot/cacheCoref-fastcoref/9984016_0.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/9984016_2.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/9984016_3.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/9984016_4.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/9984016_5.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/9984016_7.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/9984016_8.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/9984016_9.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/9984016_10.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/9984016_11.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/9984016_12.coref\n",
      "Found files... 45280\n",
      "../meetups_pilot/cacheCoref-fastcoref/45280_0.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/45280_1.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/45280_2.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/45280_3.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/45280_6.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/45280_7.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/45280_8.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/45280_9.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/45280_10.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/45280_12.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/45280_13.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/45280_14.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/45280_15.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/45280_16.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/45280_18.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/45280_19.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/45280_21.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/45280_23.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/45280_24.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/45280_25.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/45280_26.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/45280_29.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/45280_30.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/45280_31.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/45280_33.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/45280_35.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/45280_36.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/45280_37.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/45280_38.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/45280_39.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/45280_40.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/45280_41.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/45280_42.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/45280_43.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/45280_44.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/45280_46.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/45280_47.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/45280_48.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/45280_49.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/45280_50.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/45280_52.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/45280_53.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/45280_54.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/45280_55.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/45280_56.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/45280_57.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/45280_58.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/45280_59.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/45280_60.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/45280_61.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/45280_62.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/45280_63.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/45280_64.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/45280_66.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/45280_67.coref\n",
      "No files found.\n",
      "Found files... 1247969\n",
      "../meetups_pilot/cacheCoref-fastcoref/1247969_0.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1247969_1.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1247969_2.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1247969_4.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1247969_5.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1247969_6.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1247969_8.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1247969_10.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1247969_12.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1247969_14.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1247969_16.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1247969_17.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1247969_20.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1247969_25.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1247969_26.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1247969_27.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1247969_28.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1247969_29.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1247969_32.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1247969_34.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1247969_35.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1247969_36.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1247969_40.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1247969_41.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1247969_42.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1247969_43.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1247969_44.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1247969_45.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1247969_47.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1247969_48.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1247969_49.coref\n",
      "Found files... 2314058\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_0.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_2.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_3.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_4.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_6.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_7.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_8.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_9.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_11.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_12.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_13.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_14.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_15.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_16.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_17.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_18.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_19.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_20.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_21.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_22.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_23.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_24.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_25.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_26.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_27.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_28.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_29.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_30.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_31.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_32.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_34.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_35.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_36.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_37.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_38.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_39.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_40.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_41.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_42.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_43.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_44.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_45.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_47.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_48.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_49.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_50.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_51.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_52.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_53.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_54.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_55.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_56.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_57.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_58.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_59.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_60.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_61.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_62.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_63.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_64.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_65.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_66.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_67.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_68.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_69.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_70.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_72.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_73.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_74.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_75.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_76.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_77.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_78.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_79.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_81.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_83.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_84.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_85.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_87.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/2314058_89.coref\n",
      "Found files... 1958823\n",
      "../meetups_pilot/cacheCoref-fastcoref/1958823_9.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1958823_0.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1958823_2.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1958823_3.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1958823_4.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1958823_5.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1958823_6.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1958823_7.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1958823_8.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1958823_10.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1958823_12.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1958823_13.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1958823_14.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1958823_15.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1958823_16.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1958823_17.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1958823_18.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1958823_19.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1958823_20.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1958823_21.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1958823_22.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1958823_23.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1958823_24.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1958823_26.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1958823_27.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1958823_28.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1958823_29.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1958823_30.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1958823_31.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1958823_32.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1958823_34.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1958823_35.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1958823_36.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1958823_37.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1958823_38.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1958823_39.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1958823_40.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1958823_41.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1958823_43.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1958823_45.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1958823_46.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/1958823_47.coref\n",
      "Found files... 158003\n",
      "../meetups_pilot/cacheCoref-fastcoref/158003_0.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/158003_2.coref\n",
      "Found files... 30363583\n",
      "../meetups_pilot/cacheCoref-fastcoref/30363583_0.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/30363583_2.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/30363583_3.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/30363583_4.coref\n",
      "../meetups_pilot/cacheCoref-fastcoref/30363583_5.coref\n"
     ]
    }
   ],
   "source": [
    "# For this development we use the coference objects created while executing the MEETUPS pipeline\n",
    "# it is also possible to process the text and obtain the object if other/new text has to be process\n",
    "folder_name = '../meetups_pilot/cacheCoref-fastcoref'\n",
    "# list of coreferenced texts\n",
    "no_found = []\n",
    "for bio in df.itertuples():\n",
    "    coref_text = []\n",
    "    file_name_pattern = str(bio.dbpedia_id)+'_*.coref'\n",
    "    found_files = find_files(folder_name, file_name_pattern)\n",
    "    if found_files:\n",
    "        print(\"Found files... \"+str(bio.dbpedia_id))\n",
    "        for file_path in found_files:\n",
    "            print(file_path)\n",
    "\n",
    "            try:\n",
    "                # spacy object, including the coreference step in the pipeline\n",
    "                doc = Doc(nlp.vocab).from_disk(file_path)\n",
    "                # paragraph number, extracted from the file_path \n",
    "                paragraph_number = int(re.search(r'_(\\d+)\\.coref', file_path).group(1)) if re.search(r'_(\\d+)\\.coref', file_path) else None\n",
    "                coref_text.append([doc._.resolved_text,paragraph_number])\n",
    "            except Exception as e: \n",
    "                print(\"ERROR: \", bio)\n",
    "                print(e)\n",
    "                continue\n",
    "    else:\n",
    "        print(\"No files found.\")\n",
    "        no_found.append(str(bio.dbpedia_id))\n",
    "\n",
    "    # save results\n",
    "    if len(coref_text) > 0:\n",
    "        # Create a DataFrame from the list of lists\n",
    "        df_result = pd.DataFrame(coref_text, columns=['coref_text', 'paragraph_number'])\n",
    "        # Save the DataFrame to a CSV file\n",
    "        df_result.to_csv('results-coref-text/'+str(bio.dbpedia_id)+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "651df862-c22b-4bc3-a458-54317c504817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "['2444917', '10085', '21511', '2843958', '45181']\n"
     ]
    }
   ],
   "source": [
    "print(len(no_found))\n",
    "print(no_found)\n",
    "# Create a DataFrame from the list of lists\n",
    "df_no_found = pd.DataFrame(no_found, columns=['dbpedia_id'])\n",
    "# Save the DataFrame to a CSV file\n",
    "df_no_found.to_csv('no-found-list.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47bdf80a-6c28-4a81-8265-2f2848bb608a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add original text\n",
    "for bio in df.itertuples():\n",
    "    try:\n",
    "        # read indexed paragraphs\n",
    "        df = pd.read_csv('../meetups_pilot/indexedParagraphs/'+str(bio.dbpedia_id)+'.csv')\n",
    "        df.drop(columns=['section'], inplace=True)\n",
    "        df.rename(columns={'paragraph': 'ori_text'}, inplace=True)\n",
    "        df_coref = pd.read_csv('results-coref-text/'+str(bio.dbpedia_id)+'.csv')\n",
    "        df_coref.rename(columns={'paragraph_number': 'paragraphIndex'}, inplace=True)\n",
    "        \n",
    "        merged_df = pd.merge(df, df_coref, on='paragraphIndex')\n",
    "        merged_df.to_csv('results-coref-text/'+str(bio.dbpedia_id)+'.csv', index=False)\n",
    "    except Exception as e: \n",
    "        print(\"ERROR: \", bio)\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63ef0153-b6e6-4122-9c77-13b81f082896",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:  183747\n",
      "[Errno 2] No such file or directory: '../meetups_pilot/indexedParagraphs/183747.csv'\n",
      "ERROR:  45190\n",
      "[Errno 2] No such file or directory: '../meetups_pilot/indexedParagraphs/45190.csv'\n",
      "ERROR:  328911\n",
      "[Errno 2] No such file or directory: '../meetups_pilot/indexedParagraphs/328911.csv'\n",
      "ERROR:  10823\n",
      "[Errno 2] No such file or directory: '../meetups_pilot/indexedParagraphs/10823.csv'\n",
      "ERROR:  14135\n",
      "[Errno 2] No such file or directory: '../meetups_pilot/indexedParagraphs/14135.csv'\n",
      "ERROR:  12775\n",
      "[Errno 2] No such file or directory: '../meetups_pilot/indexedParagraphs/12775.csv'\n",
      "ERROR:  44981\n",
      "[Errno 2] No such file or directory: '../meetups_pilot/indexedParagraphs/44981.csv'\n",
      "ERROR:  27808\n",
      "[Errno 2] No such file or directory: '../meetups_pilot/indexedParagraphs/27808.csv'\n",
      "ERROR:  631987\n",
      "[Errno 2] No such file or directory: '../meetups_pilot/indexedParagraphs/631987.csv'\n",
      "ERROR:  339480\n",
      "[Errno 2] No such file or directory: '../meetups_pilot/indexedParagraphs/339480.csv'\n",
      "ERROR:  214031\n",
      "[Errno 2] No such file or directory: '../meetups_pilot/indexedParagraphs/214031.csv'\n",
      "ERROR:  358868\n",
      "[Errno 2] No such file or directory: '../meetups_pilot/indexedParagraphs/358868.csv'\n",
      "ERROR:  2064\n",
      "[Errno 2] No such file or directory: '../meetups_pilot/indexedParagraphs/2064.csv'\n",
      "ERROR:  77226\n",
      "[Errno 2] No such file or directory: '../meetups_pilot/indexedParagraphs/77226.csv'\n",
      "ERROR:  304067\n",
      "[Errno 2] No such file or directory: '../meetups_pilot/indexedParagraphs/304067.csv'\n",
      "ERROR:  73087\n",
      "[Errno 2] No such file or directory: '../meetups_pilot/indexedParagraphs/73087.csv'\n",
      "ERROR:  242372\n",
      "[Errno 2] No such file or directory: '../meetups_pilot/indexedParagraphs/242372.csv'\n",
      "ERROR:  99339\n",
      "[Errno 2] No such file or directory: '../meetups_pilot/indexedParagraphs/99339.csv'\n",
      "ERROR:  37944\n",
      "[Errno 2] No such file or directory: '../meetups_pilot/indexedParagraphs/37944.csv'\n",
      "ERROR:  32897\n",
      "[Errno 2] No such file or directory: '../meetups_pilot/indexedParagraphs/32897.csv'\n",
      "ERROR:  355346\n",
      "[Errno 2] No such file or directory: '../meetups_pilot/indexedParagraphs/355346.csv'\n",
      "ERROR:  57550\n",
      "[Errno 2] No such file or directory: '../meetups_pilot/indexedParagraphs/57550.csv'\n",
      "ERROR:  162325\n",
      "[Errno 2] No such file or directory: '../meetups_pilot/indexedParagraphs/162325.csv'\n",
      "ERROR:  16094\n",
      "[Errno 2] No such file or directory: '../meetups_pilot/indexedParagraphs/16094.csv'\n",
      "ERROR:  46235\n",
      "[Errno 2] No such file or directory: '../meetups_pilot/indexedParagraphs/46235.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:34:47 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 121.30 examples/s]\n",
      "01/12/2024 15:34:51 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_0.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:34:52 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 71.32 examples/s]\n",
      "01/12/2024 15:34:56 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 14.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_1.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:34:56 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 146.67 examples/s]\n",
      "01/12/2024 15:35:00 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_2.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:35:01 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 150.61 examples/s]\n",
      "01/12/2024 15:35:04 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.51it/s]\n",
      "01/12/2024 15:35:05 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_3.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 156.41 examples/s]\n",
      "01/12/2024 15:35:08 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 40.87it/s]\n",
      "01/12/2024 15:35:08 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_4.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 161.97 examples/s]\n",
      "01/12/2024 15:35:11 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 39.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_5.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:35:14 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 111.12 examples/s]\n",
      "01/12/2024 15:35:18 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_6.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:35:20 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 113.56 examples/s]\n",
      "01/12/2024 15:35:24 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_7.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:35:25 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 120.54 examples/s]\n",
      "01/12/2024 15:35:29 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 22.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_8.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:35:31 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 85.43 examples/s]\n",
      "01/12/2024 15:35:35 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.34it/s]\n",
      "01/12/2024 15:35:35 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_9.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 154.49 examples/s]\n",
      "01/12/2024 15:35:38 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 21.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_10.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:35:39 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 91.81 examples/s]\n",
      "01/12/2024 15:35:43 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_11.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:35:44 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 119.10 examples/s]\n",
      "01/12/2024 15:35:48 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_12.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:35:48 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 159.66 examples/s]\n",
      "01/12/2024 15:35:51 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 24.83it/s]\n",
      "01/12/2024 15:35:52 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_13.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 161.79 examples/s]\n",
      "01/12/2024 15:35:55 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 24.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_14.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:35:56 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 105.04 examples/s]\n",
      "01/12/2024 15:36:00 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.87it/s]\n",
      "01/12/2024 15:36:00 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_15.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 156.13 examples/s]\n",
      "01/12/2024 15:36:04 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 34.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_16.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:36:06 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 123.10 examples/s]\n",
      "01/12/2024 15:36:09 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.15it/s]\n",
      "01/12/2024 15:36:10 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_17.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 161.95 examples/s]\n",
      "01/12/2024 15:36:13 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 39.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_18.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:36:13 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 144.59 examples/s]\n",
      "01/12/2024 15:36:17 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.76it/s]\n",
      "01/12/2024 15:36:17 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_19.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 162.97 examples/s]\n",
      "01/12/2024 15:36:20 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 40.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_20.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:36:21 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 134.48 examples/s]\n",
      "01/12/2024 15:36:24 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 21.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_21.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:36:25 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 111.41 examples/s]\n",
      "01/12/2024 15:36:29 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_22.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:36:30 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 140.60 examples/s]\n",
      "01/12/2024 15:36:33 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_23.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:36:34 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 128.77 examples/s]\n",
      "01/12/2024 15:36:38 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_24.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:36:39 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 158.17 examples/s]\n",
      "01/12/2024 15:36:42 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 17.42it/s]\n",
      "01/12/2024 15:36:43 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_25.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 163.44 examples/s]\n",
      "01/12/2024 15:36:46 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 40.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_26.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:36:46 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 157.92 examples/s]\n",
      "01/12/2024 15:36:50 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 20.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_27.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:36:50 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 139.37 examples/s]\n",
      "01/12/2024 15:36:54 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_28.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:36:54 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 147.73 examples/s]\n",
      "01/12/2024 15:36:58 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 14.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_29.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:36:59 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 163.22 examples/s]\n",
      "01/12/2024 15:37:03 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 19.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_30.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:37:04 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 120.03 examples/s]\n",
      "01/12/2024 15:37:08 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_31.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:37:09 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 124.62 examples/s]\n",
      "01/12/2024 15:37:12 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_32.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:37:13 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 111.91 examples/s]\n",
      "01/12/2024 15:37:17 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_33.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:37:19 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 148.79 examples/s]\n",
      "01/12/2024 15:37:22 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 22.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_34.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:37:23 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 103.57 examples/s]\n",
      "01/12/2024 15:37:27 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_35.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:37:28 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 131.45 examples/s]\n",
      "01/12/2024 15:37:32 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.42it/s]\n",
      "01/12/2024 15:37:32 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_36.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 156.18 examples/s]\n",
      "01/12/2024 15:37:35 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 41.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_37.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:37:36 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 126.79 examples/s]\n",
      "01/12/2024 15:37:40 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_38.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:37:41 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 162.53 examples/s]\n",
      "01/12/2024 15:37:44 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 13.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_39.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:37:45 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 143.07 examples/s]\n",
      "01/12/2024 15:37:49 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_40.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:37:50 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 132.69 examples/s]\n",
      "01/12/2024 15:37:53 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.61it/s]\n",
      "01/12/2024 15:37:54 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_41.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 159.51 examples/s]\n",
      "01/12/2024 15:37:57 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 39.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_42.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:38:00 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 74.34 examples/s]\n",
      "01/12/2024 15:38:03 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.33it/s]\n",
      "01/12/2024 15:38:04 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_43.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 160.79 examples/s]\n",
      "01/12/2024 15:38:07 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 33.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_44.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:38:08 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 160.88 examples/s]\n",
      "01/12/2024 15:38:11 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 20.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_45.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:38:12 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 162.93 examples/s]\n",
      "01/12/2024 15:38:15 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 22.22it/s]\n",
      "01/12/2024 15:38:15 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_46.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 160.14 examples/s]\n",
      "01/12/2024 15:38:19 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 36.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_47.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:38:20 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 167.46 examples/s]\n",
      "01/12/2024 15:38:23 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 17.14it/s]\n",
      "01/12/2024 15:38:23 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_48.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 161.63 examples/s]\n",
      "01/12/2024 15:38:27 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 39.05it/s]\n",
      "01/12/2024 15:38:27 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_49.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 162.68 examples/s]\n",
      "01/12/2024 15:38:30 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 38.50it/s]\n",
      "01/12/2024 15:38:30 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_50.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 165.44 examples/s]\n",
      "01/12/2024 15:38:33 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 37.87it/s]\n",
      "01/12/2024 15:38:33 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_51.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 136.20 examples/s]\n",
      "01/12/2024 15:38:37 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 37.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_52.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:38:37 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 174.73 examples/s]\n",
      "01/12/2024 15:38:41 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 23.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_53.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:38:42 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 170.28 examples/s]\n",
      "01/12/2024 15:38:46 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 25.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_54.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:38:47 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 157.47 examples/s]\n",
      "01/12/2024 15:38:51 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 13.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_55.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:38:51 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 183.74 examples/s]\n",
      "01/12/2024 15:38:54 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 34.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_56.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:38:55 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 183.07 examples/s]\n",
      "01/12/2024 15:38:58 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 36.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_57.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:38:59 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 174.11 examples/s]\n",
      "01/12/2024 15:39:02 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 32.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_58.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:39:03 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 159.61 examples/s]\n",
      "01/12/2024 15:39:07 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 29.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_59.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:39:08 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 164.29 examples/s]\n",
      "01/12/2024 15:39:12 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 27.80it/s]\n",
      "01/12/2024 15:39:12 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_60.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 162.00 examples/s]\n",
      "01/12/2024 15:39:15 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 40.35it/s]\n",
      "01/12/2024 15:39:15 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_61.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 164.43 examples/s]\n",
      "01/12/2024 15:39:18 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 40.04it/s]\n",
      "01/12/2024 15:39:19 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_62.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 161.53 examples/s]\n",
      "01/12/2024 15:39:22 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 40.08it/s]\n",
      "01/12/2024 15:39:22 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_63.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 174.61 examples/s]\n",
      "01/12/2024 15:39:25 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 39.99it/s]\n",
      "01/12/2024 15:39:25 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_64.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 162.48 examples/s]\n",
      "01/12/2024 15:39:28 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 38.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_65.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:39:28 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 166.20 examples/s]\n",
      "01/12/2024 15:39:31 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 36.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_66.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:39:32 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 162.87 examples/s]\n",
      "01/12/2024 15:39:35 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 38.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_67.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:39:35 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 167.84 examples/s]\n",
      "01/12/2024 15:39:38 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 30.21it/s]\n",
      "01/12/2024 15:39:38 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_68.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 161.20 examples/s]\n",
      "01/12/2024 15:39:41 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 41.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_69.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:39:42 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 174.92 examples/s]\n",
      "01/12/2024 15:39:45 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 37.72it/s]\n",
      "01/12/2024 15:39:45 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_70.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 166.33 examples/s]\n",
      "01/12/2024 15:39:48 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 40.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_71.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:39:48 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 164.70 examples/s]\n",
      "01/12/2024 15:39:51 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 35.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_72.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:39:52 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 176.48 examples/s]\n",
      "01/12/2024 15:39:55 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 35.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_73.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:39:55 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 165.45 examples/s]\n",
      "01/12/2024 15:39:58 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 40.33it/s]\n",
      "01/12/2024 15:39:58 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_74.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 167.46 examples/s]\n",
      "01/12/2024 15:40:01 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 38.06it/s]\n",
      "01/12/2024 15:40:01 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_75.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 162.94 examples/s]\n",
      "01/12/2024 15:40:04 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 36.33it/s]\n",
      "01/12/2024 15:40:05 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_76.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 175.19 examples/s]\n",
      "01/12/2024 15:40:08 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 39.08it/s]\n",
      "01/12/2024 15:40:08 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_77.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 162.87 examples/s]\n",
      "01/12/2024 15:40:11 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 41.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  44887_78.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:40:11 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 154.69 examples/s]\n",
      "01/12/2024 15:40:14 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 16.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  2444917_0.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:40:15 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 162.39 examples/s]\n",
      "01/12/2024 15:40:18 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 18.51it/s]\n",
      "01/12/2024 15:40:18 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  2444917_1.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 164.42 examples/s]\n",
      "01/12/2024 15:40:21 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 39.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  2444917_2.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:40:23 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 138.99 examples/s]\n",
      "01/12/2024 15:40:26 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  2444917_3.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:40:29 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 113.57 examples/s]\n",
      "01/12/2024 15:40:32 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.88it/s]\n",
      "01/12/2024 15:40:33 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  2444917_4.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 150.49 examples/s]\n",
      "01/12/2024 15:40:36 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 38.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  2444917_5.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:40:38 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 122.57 examples/s]\n",
      "01/12/2024 15:40:42 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  2444917_6.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:40:43 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 163.11 examples/s]\n",
      "01/12/2024 15:40:46 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 21.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  2444917_7.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:40:47 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 82.66 examples/s]\n",
      "01/12/2024 15:40:51 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  10085_0.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:40:53 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 132.68 examples/s]\n",
      "01/12/2024 15:40:56 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  10085_1.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:40:57 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 145.14 examples/s]\n",
      "01/12/2024 15:41:01 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 15.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  10085_2.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:41:01 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 163.63 examples/s]\n",
      "01/12/2024 15:41:04 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 21.63it/s]\n",
      "01/12/2024 15:41:05 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  10085_3.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 164.01 examples/s]\n",
      "01/12/2024 15:41:08 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 40.40it/s]\n",
      "01/12/2024 15:41:08 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  10085_4.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 166.05 examples/s]\n",
      "01/12/2024 15:41:12 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 37.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  10085_5.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:41:13 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 121.03 examples/s]\n",
      "01/12/2024 15:41:17 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  10085_6.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:41:19 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 67.16 examples/s]\n",
      "01/12/2024 15:41:22 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  10085_7.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:41:24 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 104.29 examples/s]\n",
      "01/12/2024 15:41:29 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.88it/s]\n",
      "01/12/2024 15:41:29 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  10085_8.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 112.30 examples/s]\n",
      "01/12/2024 15:41:33 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 31.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  10085_9.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:41:34 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 106.84 examples/s]\n",
      "01/12/2024 15:41:38 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  10085_10.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:41:39 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 103.74 examples/s]\n",
      "01/12/2024 15:41:42 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.33it/s]\n",
      "01/12/2024 15:41:43 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  10085_11.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 131.15 examples/s]\n",
      "01/12/2024 15:41:45 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 38.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  10085_12.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:41:47 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 73.32 examples/s]\n",
      "01/12/2024 15:41:51 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  10085_13.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:41:52 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 100.99 examples/s]\n",
      "01/12/2024 15:41:56 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.91it/s]\n",
      "01/12/2024 15:41:56 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  10085_14.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 160.03 examples/s]\n",
      "01/12/2024 15:41:59 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 35.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  10085_15.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:42:01 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 85.24 examples/s]\n",
      "01/12/2024 15:42:05 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  10085_16.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:42:07 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 86.27 examples/s]\n",
      "01/12/2024 15:42:10 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  10085_17.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:42:13 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 84.87 examples/s]\n",
      "01/12/2024 15:42:17 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  10085_18.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:42:19 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 97.78 examples/s]\n",
      "01/12/2024 15:42:22 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.77it/s]\n",
      "01/12/2024 15:42:23 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  10085_19.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 174.29 examples/s]\n",
      "01/12/2024 15:42:26 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 37.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  10085_20.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:42:27 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 118.48 examples/s]\n",
      "01/12/2024 15:42:31 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  10085_21.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:42:32 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 108.01 examples/s]\n",
      "01/12/2024 15:42:36 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  10085_22.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:42:37 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 126.31 examples/s]\n",
      "01/12/2024 15:42:41 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.84it/s]\n",
      "01/12/2024 15:42:41 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  10085_23.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 165.64 examples/s]\n",
      "01/12/2024 15:42:44 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 36.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  10085_24.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:42:47 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 67.87 examples/s]\n",
      "01/12/2024 15:42:51 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  10085_25.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:42:52 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 122.52 examples/s]\n",
      "01/12/2024 15:42:56 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  10085_26.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:42:57 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 107.54 examples/s]\n",
      "01/12/2024 15:43:01 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.77it/s]\n",
      "01/12/2024 15:43:02 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  10085_27.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 161.20 examples/s]\n",
      "01/12/2024 15:43:05 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 37.02it/s]\n",
      "01/12/2024 15:43:05 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  10085_28.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 164.61 examples/s]\n",
      "01/12/2024 15:43:08 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 35.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  10085_29.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:43:09 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 81.81 examples/s]\n",
      "01/12/2024 15:43:13 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.70it/s]\n",
      "01/12/2024 15:43:13 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  10085_30.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 164.33 examples/s]\n",
      "01/12/2024 15:43:17 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 37.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  10085_31.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:43:18 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 93.84 examples/s]\n",
      "01/12/2024 15:43:21 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  10085_32.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:43:23 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 89.45 examples/s]\n",
      "01/12/2024 15:43:27 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.42it/s]\n",
      "01/12/2024 15:43:27 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  10085_33.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 174.39 examples/s]\n",
      "01/12/2024 15:43:31 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 24.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  10085_34.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:43:32 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 131.45 examples/s]\n",
      "01/12/2024 15:43:35 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.75it/s]\n",
      "01/12/2024 15:43:36 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  10085_35.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 158.83 examples/s]\n",
      "01/12/2024 15:43:39 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 40.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  10085_36.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:43:40 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 104.13 examples/s]\n",
      "01/12/2024 15:43:44 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  10085_37.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:43:45 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 128.61 examples/s]\n",
      "01/12/2024 15:43:47 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  10085_38.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:43:48 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 135.21 examples/s]\n",
      "01/12/2024 15:43:51 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 14.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  10085_39.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:43:54 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 75.92 examples/s]\n",
      "01/12/2024 15:43:57 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.57it/s]\n",
      "01/12/2024 15:43:58 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  10085_40.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 159.35 examples/s]\n",
      "01/12/2024 15:44:01 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 40.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  10085_41.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "01/12/2024 15:44:05 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 76.80 examples/s]\n",
      "01/12/2024 15:44:09 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  10085_42.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:44:11 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 98.79 examples/s]\n",
      "01/12/2024 15:44:15 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  10085_43.coref\n",
      "ERROR:  256279\n",
      "[Errno 2] No such file or directory: '../meetups_pilot/indexedParagraphs/256279.csv'\n",
      "ERROR:  63705\n",
      "[Errno 2] No such file or directory: '../meetups_pilot/indexedParagraphs/63705.csv'\n",
      "ERROR:  9330\n",
      "[Errno 2] No such file or directory: '../meetups_pilot/indexedParagraphs/9330.csv'\n",
      "ERROR:  12963\n",
      "[Errno 2] No such file or directory: '../meetups_pilot/indexedParagraphs/12963.csv'\n",
      "ERROR:  28986\n",
      "[Errno 2] No such file or directory: '../meetups_pilot/indexedParagraphs/28986.csv'\n",
      "ERROR:  3180810\n",
      "[Errno 2] No such file or directory: '../meetups_pilot/indexedParagraphs/3180810.csv'\n",
      "ERROR:  44660\n",
      "[Errno 2] No such file or directory: '../meetups_pilot/indexedParagraphs/44660.csv'\n",
      "ERROR:  44172\n",
      "[Errno 2] No such file or directory: '../meetups_pilot/indexedParagraphs/44172.csv'\n",
      "ERROR:  2627935\n",
      "[Errno 2] No such file or directory: '../meetups_pilot/indexedParagraphs/2627935.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:44:16 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 155.26 examples/s]\n",
      "01/12/2024 15:44:20 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 13.33it/s]\n",
      "01/12/2024 15:44:20 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_0.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 163.32 examples/s]\n",
      "01/12/2024 15:44:23 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 41.04it/s]\n",
      "01/12/2024 15:44:23 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_1.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 161.80 examples/s]\n",
      "01/12/2024 15:44:26 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 41.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_2.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:44:27 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 137.41 examples/s]\n",
      "01/12/2024 15:44:31 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.96it/s]\n",
      "01/12/2024 15:44:31 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_3.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 159.13 examples/s]\n",
      "01/12/2024 15:44:34 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 41.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_4.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:44:35 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 143.99 examples/s]\n",
      "01/12/2024 15:44:38 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 13.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_5.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:44:39 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 141.31 examples/s]\n",
      "01/12/2024 15:44:43 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 17.18it/s]\n",
      "01/12/2024 15:44:43 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_6.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 162.40 examples/s]\n",
      "01/12/2024 15:44:46 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 39.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_7.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:44:47 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 152.29 examples/s]\n",
      "01/12/2024 15:44:51 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 16.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_8.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:44:52 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 149.10 examples/s]\n",
      "01/12/2024 15:44:55 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 15.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_9.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:44:56 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 160.39 examples/s]\n",
      "01/12/2024 15:44:59 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 23.07it/s]\n",
      "01/12/2024 15:44:59 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_10.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 78.71 examples/s]\n",
      "01/12/2024 15:45:03 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 22.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_11.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:45:03 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 140.82 examples/s]\n",
      "01/12/2024 15:45:07 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_12.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:45:08 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 119.87 examples/s]\n",
      "01/12/2024 15:45:12 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.78it/s]\n",
      "01/12/2024 15:45:12 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_13.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 178.88 examples/s]\n",
      "01/12/2024 15:45:15 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 41.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_14.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:45:17 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 125.21 examples/s]\n",
      "01/12/2024 15:45:21 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.78it/s]\n",
      "01/12/2024 15:45:21 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_15.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 163.76 examples/s]\n",
      "01/12/2024 15:45:24 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 41.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_16.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:45:24 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 151.21 examples/s]\n",
      "01/12/2024 15:45:28 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 16.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_17.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:45:29 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 154.12 examples/s]\n",
      "01/12/2024 15:45:32 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 15.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_18.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:45:33 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 118.98 examples/s]\n",
      "01/12/2024 15:45:36 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 13.05it/s]\n",
      "01/12/2024 15:45:37 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_19.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 183.89 examples/s]\n",
      "01/12/2024 15:45:40 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 41.98it/s]\n",
      "01/12/2024 15:45:40 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_20.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 161.10 examples/s]\n",
      "01/12/2024 15:45:43 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 39.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_21.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:45:44 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 146.39 examples/s]\n",
      "01/12/2024 15:45:48 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 13.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_22.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:45:49 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 125.67 examples/s]\n",
      "01/12/2024 15:45:53 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 13.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_23.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:45:53 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 150.62 examples/s]\n",
      "01/12/2024 15:45:57 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 22.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_24.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:45:59 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 146.89 examples/s]\n",
      "01/12/2024 15:46:03 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 22.24it/s]\n",
      "01/12/2024 15:46:03 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_25.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 123.04 examples/s]\n",
      "01/12/2024 15:46:06 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 28.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_26.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:46:07 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 127.26 examples/s]\n",
      "01/12/2024 15:46:11 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 13.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_27.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:46:12 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 130.19 examples/s]\n",
      "01/12/2024 15:46:15 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_28.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:46:16 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 164.28 examples/s]\n",
      "01/12/2024 15:46:19 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 25.09it/s]\n",
      "01/12/2024 15:46:19 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_29.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 139.96 examples/s]\n",
      "01/12/2024 15:46:23 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 34.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_30.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:46:24 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 145.15 examples/s]\n",
      "01/12/2024 15:46:28 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 13.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_31.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:46:28 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 152.80 examples/s]\n",
      "01/12/2024 15:46:32 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 15.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_32.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:46:32 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 136.97 examples/s]\n",
      "01/12/2024 15:46:36 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.10it/s]\n",
      "01/12/2024 15:46:36 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_33.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 184.10 examples/s]\n",
      "01/12/2024 15:46:39 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 32.95it/s]\n",
      "01/12/2024 15:46:39 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_34.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 180.38 examples/s]\n",
      "01/12/2024 15:46:42 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 42.31it/s]\n",
      "01/12/2024 15:46:42 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_35.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 147.55 examples/s]\n",
      "01/12/2024 15:46:45 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 39.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_36.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:46:46 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 171.01 examples/s]\n",
      "01/12/2024 15:46:49 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 41.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_37.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:46:49 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 177.24 examples/s]\n",
      "01/12/2024 15:46:52 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 38.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_38.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:46:52 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 175.77 examples/s]\n",
      "01/12/2024 15:46:56 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 36.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_39.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:46:56 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 162.66 examples/s]\n",
      "01/12/2024 15:46:59 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 37.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_40.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:46:59 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 147.19 examples/s]\n",
      "01/12/2024 15:47:03 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 29.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_41.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:47:04 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 95.13 examples/s]\n",
      "01/12/2024 15:47:07 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 24.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_42.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:47:08 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 172.78 examples/s]\n",
      "01/12/2024 15:47:11 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 32.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_43.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:47:12 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 172.43 examples/s]\n",
      "01/12/2024 15:47:15 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 32.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_44.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:47:16 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 162.58 examples/s]\n",
      "01/12/2024 15:47:19 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 28.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_45.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:47:19 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 160.92 examples/s]\n",
      "01/12/2024 15:47:23 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 32.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_46.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:47:23 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 150.05 examples/s]\n",
      "01/12/2024 15:47:26 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 20.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_47.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:47:27 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 158.07 examples/s]\n",
      "01/12/2024 15:47:31 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 18.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_48.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:47:31 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 164.11 examples/s]\n",
      "01/12/2024 15:47:34 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 26.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_49.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:47:35 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 153.24 examples/s]\n",
      "01/12/2024 15:47:39 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 15.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_50.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:47:40 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 163.64 examples/s]\n",
      "01/12/2024 15:47:43 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 34.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_51.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:47:44 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 164.13 examples/s]\n",
      "01/12/2024 15:47:47 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 28.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_52.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:47:47 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 166.00 examples/s]\n",
      "01/12/2024 15:47:50 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 33.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_53.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:47:51 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 159.18 examples/s]\n",
      "01/12/2024 15:47:54 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 22.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_54.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:47:55 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 165.27 examples/s]\n",
      "01/12/2024 15:47:58 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 24.56it/s]\n",
      "01/12/2024 15:47:58 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_55.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 164.53 examples/s]\n",
      "01/12/2024 15:48:01 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 34.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_56.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:48:01 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 158.75 examples/s]\n",
      "01/12/2024 15:48:05 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 31.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_57.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:48:06 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 122.48 examples/s]\n",
      "01/12/2024 15:48:08 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 32.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_58.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:48:09 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 157.76 examples/s]\n",
      "01/12/2024 15:48:12 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 30.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_59.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:48:13 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 145.60 examples/s]\n",
      "01/12/2024 15:48:16 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 16.88it/s]\n",
      "01/12/2024 15:48:17 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_60.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 162.55 examples/s]\n",
      "01/12/2024 15:48:20 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 40.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_61.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:48:20 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 173.64 examples/s]\n",
      "01/12/2024 15:48:24 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 31.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_62.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:48:26 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 160.82 examples/s]\n",
      "01/12/2024 15:48:29 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.53it/s]\n",
      "01/12/2024 15:48:30 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_63.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 183.44 examples/s]\n",
      "01/12/2024 15:48:33 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 37.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_64.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:48:33 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 143.30 examples/s]\n",
      "01/12/2024 15:48:37 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 17.57it/s]\n",
      "01/12/2024 15:48:37 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_65.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 150.63 examples/s]\n",
      "01/12/2024 15:48:41 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 39.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_66.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:48:43 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 148.74 examples/s]\n",
      "01/12/2024 15:48:46 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 19.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_67.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:48:48 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 135.73 examples/s]\n",
      "01/12/2024 15:48:51 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_68.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:48:53 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 109.42 examples/s]\n",
      "01/12/2024 15:48:57 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.24it/s]\n",
      "01/12/2024 15:48:57 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_69.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 153.81 examples/s]\n",
      "01/12/2024 15:49:01 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 33.99it/s]\n",
      "01/12/2024 15:49:01 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_70.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 186.72 examples/s]\n",
      "01/12/2024 15:49:05 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 44.79it/s]\n",
      "01/12/2024 15:49:05 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_71.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 165.49 examples/s]\n",
      "01/12/2024 15:49:08 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 30.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_72.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:49:09 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 137.76 examples/s]\n",
      "01/12/2024 15:49:12 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 30.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_73.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:49:12 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 142.16 examples/s]\n",
      "01/12/2024 15:49:16 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 36.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_74.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:49:16 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 146.84 examples/s]\n",
      "01/12/2024 15:49:19 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 31.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_75.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:49:20 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 163.79 examples/s]\n",
      "01/12/2024 15:49:23 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 29.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_76.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:49:24 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 93.89 examples/s]\n",
      "01/12/2024 15:49:27 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 25.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_77.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:49:28 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 157.85 examples/s]\n",
      "01/12/2024 15:49:32 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 32.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_78.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:49:32 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 180.98 examples/s]\n",
      "01/12/2024 15:49:35 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 38.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_79.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:49:36 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 158.29 examples/s]\n",
      "01/12/2024 15:49:39 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 28.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_80.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:49:40 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 78.61 examples/s]\n",
      "01/12/2024 15:49:43 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 23.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_81.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:49:43 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 143.72 examples/s]\n",
      "01/12/2024 15:49:47 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 33.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_82.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:49:48 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 159.77 examples/s]\n",
      "01/12/2024 15:49:51 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 18.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_83.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:49:52 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 161.33 examples/s]\n",
      "01/12/2024 15:49:55 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 27.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_84.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:49:56 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 101.94 examples/s]\n",
      "01/12/2024 15:50:00 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 16.56it/s]\n",
      "01/12/2024 15:50:01 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_85.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 159.21 examples/s]\n",
      "01/12/2024 15:50:04 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 40.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_86.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:50:04 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 166.92 examples/s]\n",
      "01/12/2024 15:50:08 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 33.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_87.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:50:08 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 151.50 examples/s]\n",
      "01/12/2024 15:50:11 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 43.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_88.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:50:12 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 160.86 examples/s]\n",
      "01/12/2024 15:50:15 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 22.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_89.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:50:15 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 163.09 examples/s]\n",
      "01/12/2024 15:50:18 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 36.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_90.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:50:19 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 160.12 examples/s]\n",
      "01/12/2024 15:50:22 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 28.86it/s]\n",
      "01/12/2024 15:50:22 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_91.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 157.44 examples/s]\n",
      "01/12/2024 15:50:25 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 45.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  21511_92.coref\n",
      "ERROR:  46384\n",
      "[Errno 2] No such file or directory: '../meetups_pilot/indexedParagraphs/46384.csv'\n",
      "ERROR:  161137\n",
      "[Errno 2] No such file or directory: '../meetups_pilot/indexedParagraphs/161137.csv'\n",
      "ERROR:  923144\n",
      "[Errno 2] No such file or directory: '../meetups_pilot/indexedParagraphs/923144.csv'\n",
      "ERROR:  37932\n",
      "[Errno 2] No such file or directory: '../meetups_pilot/indexedParagraphs/37932.csv'\n",
      "ERROR:  991137\n",
      "[Errno 2] No such file or directory: '../meetups_pilot/indexedParagraphs/991137.csv'\n",
      "ERROR:  40225\n",
      "[Errno 2] No such file or directory: '../meetups_pilot/indexedParagraphs/40225.csv'\n",
      "ERROR:  574470\n",
      "[Errno 2] No such file or directory: '../meetups_pilot/indexedParagraphs/574470.csv'\n",
      "ERROR:  37945\n",
      "[Errno 2] No such file or directory: '../meetups_pilot/indexedParagraphs/37945.csv'\n",
      "ERROR:  32668\n",
      "[Errno 2] No such file or directory: '../meetups_pilot/indexedParagraphs/32668.csv'\n",
      "ERROR:  304080\n",
      "[Errno 2] No such file or directory: '../meetups_pilot/indexedParagraphs/304080.csv'\n",
      "ERROR:  46641\n",
      "[Errno 2] No such file or directory: '../meetups_pilot/indexedParagraphs/46641.csv'\n",
      "ERROR:  977467\n",
      "[Errno 2] No such file or directory: '../meetups_pilot/indexedParagraphs/977467.csv'\n",
      "ERROR:  65157\n",
      "[Errno 2] No such file or directory: '../meetups_pilot/indexedParagraphs/65157.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:50:26 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 152.57 examples/s]\n",
      "01/12/2024 15:50:29 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 30.68it/s]\n",
      "01/12/2024 15:50:30 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  2843958_0.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 157.83 examples/s]\n",
      "01/12/2024 15:50:33 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 44.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  2843958_1.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:50:34 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 107.14 examples/s]\n",
      "01/12/2024 15:50:38 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  2843958_2.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:50:39 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 154.71 examples/s]\n",
      "01/12/2024 15:50:42 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  2843958_3.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:50:43 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 157.93 examples/s]\n",
      "01/12/2024 15:50:47 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 21.63it/s]\n",
      "01/12/2024 15:50:47 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  2843958_4.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 157.95 examples/s]\n",
      "01/12/2024 15:50:50 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 43.60it/s]\n",
      "01/12/2024 15:50:50 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  2843958_5.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 159.62 examples/s]\n",
      "01/12/2024 15:50:53 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 44.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  2843958_6.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:50:54 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 154.25 examples/s]\n",
      "01/12/2024 15:50:58 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 20.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  2843958_7.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:50:59 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 156.97 examples/s]\n",
      "01/12/2024 15:51:02 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 22.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  2843958_8.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:51:03 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 164.06 examples/s]\n",
      "01/12/2024 15:51:07 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 20.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  2843958_9.coref\n",
      "ERROR:  209010\n",
      "[Errno 2] No such file or directory: '../meetups_pilot/indexedParagraphs/209010.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:51:07 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 167.71 examples/s]\n",
      "01/12/2024 15:51:11 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 13.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_0.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:51:12 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 168.72 examples/s]\n",
      "01/12/2024 15:51:16 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 16.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_1.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:51:17 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 159.83 examples/s]\n",
      "01/12/2024 15:51:20 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 16.65it/s]\n",
      "01/12/2024 15:51:20 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_2.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 159.07 examples/s]\n",
      "01/12/2024 15:51:23 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 38.09it/s]\n",
      "01/12/2024 15:51:23 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_3.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 158.66 examples/s]\n",
      "01/12/2024 15:51:27 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 37.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_4.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:51:29 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 116.45 examples/s]\n",
      "01/12/2024 15:51:33 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_5.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:51:34 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 109.97 examples/s]\n",
      "01/12/2024 15:51:37 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_6.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:51:38 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 151.87 examples/s]\n",
      "01/12/2024 15:51:41 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 37.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_7.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:51:43 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 100.94 examples/s]\n",
      "01/12/2024 15:51:46 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.41it/s]\n",
      "01/12/2024 15:51:47 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_8.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 164.96 examples/s]\n",
      "01/12/2024 15:51:50 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 38.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_9.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:51:51 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 126.41 examples/s]\n",
      "01/12/2024 15:51:54 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_10.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:51:55 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 158.27 examples/s]\n",
      "01/12/2024 15:51:58 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 38.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_11.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:51:59 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 125.63 examples/s]\n",
      "01/12/2024 15:52:03 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_12.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:52:04 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 156.91 examples/s]\n",
      "01/12/2024 15:52:07 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 25.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_13.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:52:09 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 83.51 examples/s]\n",
      "01/12/2024 15:52:13 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_14.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:52:13 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 159.16 examples/s]\n",
      "01/12/2024 15:52:17 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 37.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_15.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:52:19 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 86.73 examples/s]\n",
      "01/12/2024 15:52:23 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_16.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:52:23 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 155.12 examples/s]\n",
      "01/12/2024 15:52:27 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 40.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_17.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:52:28 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 92.17 examples/s]\n",
      "01/12/2024 15:52:31 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_18.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:52:32 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 132.05 examples/s]\n",
      "01/12/2024 15:52:37 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_19.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:52:37 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 161.30 examples/s]\n",
      "01/12/2024 15:52:41 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 39.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_20.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:52:42 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 102.88 examples/s]\n",
      "01/12/2024 15:52:46 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_21.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:52:47 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 143.02 examples/s]\n",
      "01/12/2024 15:52:50 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 18.25it/s]\n",
      "01/12/2024 15:52:50 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_22.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 158.72 examples/s]\n",
      "01/12/2024 15:52:53 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 41.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_23.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:52:54 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 106.08 examples/s]\n",
      "01/12/2024 15:52:57 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_24.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:52:58 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 134.87 examples/s]\n",
      "01/12/2024 15:53:02 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 16.61it/s]\n",
      "01/12/2024 15:53:02 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_25.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 147.31 examples/s]\n",
      "01/12/2024 15:53:05 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 40.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_26.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:53:06 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 164.97 examples/s]\n",
      "01/12/2024 15:53:09 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 26.30it/s]\n",
      "01/12/2024 15:53:09 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_27.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 158.65 examples/s]\n",
      "01/12/2024 15:53:12 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 39.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_28.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:53:13 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 109.28 examples/s]\n",
      "01/12/2024 15:53:17 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_29.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:53:17 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 145.51 examples/s]\n",
      "01/12/2024 15:53:21 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_30.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:53:22 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 148.11 examples/s]\n",
      "01/12/2024 15:53:25 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 14.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_31.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:53:26 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 162.27 examples/s]\n",
      "01/12/2024 15:53:29 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 23.94it/s]\n",
      "01/12/2024 15:53:29 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_32.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 164.97 examples/s]\n",
      "01/12/2024 15:53:32 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 39.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_33.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:53:33 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 147.89 examples/s]\n",
      "01/12/2024 15:53:36 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_34.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:53:37 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 141.81 examples/s]\n",
      "01/12/2024 15:53:40 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.83it/s]\n",
      "01/12/2024 15:53:40 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_35.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 161.36 examples/s]\n",
      "01/12/2024 15:53:43 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 41.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_36.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:53:47 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 144.92 examples/s]\n",
      "01/12/2024 15:53:51 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 14.42it/s]\n",
      "01/12/2024 15:53:51 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_37.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 163.75 examples/s]\n",
      "01/12/2024 15:53:54 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 41.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_38.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:53:54 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 157.73 examples/s]\n",
      "01/12/2024 15:53:58 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 22.86it/s]\n",
      "01/12/2024 15:53:58 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_39.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 163.99 examples/s]\n",
      "01/12/2024 15:54:01 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 40.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_40.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:54:02 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 132.18 examples/s]\n",
      "01/12/2024 15:54:05 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_41.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:54:06 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 143.29 examples/s]\n",
      "01/12/2024 15:54:10 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 13.34it/s]\n",
      "01/12/2024 15:54:10 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_42.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 186.77 examples/s]\n",
      "01/12/2024 15:54:13 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 42.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_43.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:54:13 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 139.21 examples/s]\n",
      "01/12/2024 15:54:17 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_44.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:54:23 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 155.13 examples/s]\n",
      "01/12/2024 15:54:27 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 22.06it/s]\n",
      "01/12/2024 15:54:27 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_45.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 183.62 examples/s]\n",
      "01/12/2024 15:54:30 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 43.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_46.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:54:31 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 110.41 examples/s]\n",
      "01/12/2024 15:54:35 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.90it/s]\n",
      "01/12/2024 15:54:35 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_47.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 156.08 examples/s]\n",
      "01/12/2024 15:54:38 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 38.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_48.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:54:39 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 135.07 examples/s]\n",
      "01/12/2024 15:54:42 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_49.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:54:43 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 124.88 examples/s]\n",
      "01/12/2024 15:54:47 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.49it/s]\n",
      "01/12/2024 15:54:47 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_50.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 157.49 examples/s]\n",
      "01/12/2024 15:54:50 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 40.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_51.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:54:50 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 129.77 examples/s]\n",
      "01/12/2024 15:54:53 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 18.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_52.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:54:54 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 141.22 examples/s]\n",
      "01/12/2024 15:54:57 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 14.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_53.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:54:58 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 151.64 examples/s]\n",
      "01/12/2024 15:55:01 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 16.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_54.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:55:01 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 177.35 examples/s]\n",
      "01/12/2024 15:55:05 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 35.85it/s]\n",
      "01/12/2024 15:55:05 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_55.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 146.23 examples/s]\n",
      "01/12/2024 15:55:09 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 17.95it/s]\n",
      "01/12/2024 15:55:09 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_56.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 140.95 examples/s]\n",
      "01/12/2024 15:55:12 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 36.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_57.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:55:13 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 98.48 examples/s]\n",
      "01/12/2024 15:55:17 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.27it/s]\n",
      "01/12/2024 15:55:17 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_58.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 169.47 examples/s]\n",
      "01/12/2024 15:55:20 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 41.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_59.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:55:21 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 158.68 examples/s]\n",
      "01/12/2024 15:55:24 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 16.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_60.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:55:25 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 147.97 examples/s]\n",
      "01/12/2024 15:55:28 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_61.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:55:29 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 157.89 examples/s]\n",
      "01/12/2024 15:55:32 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 16.21it/s]\n",
      "01/12/2024 15:55:32 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_62.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 169.64 examples/s]\n",
      "01/12/2024 15:55:35 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 40.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_63.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:55:36 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 144.03 examples/s]\n",
      "01/12/2024 15:55:39 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_64.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:55:40 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 150.65 examples/s]\n",
      "01/12/2024 15:55:43 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 15.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_65.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:55:44 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 130.70 examples/s]\n",
      "01/12/2024 15:55:47 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 14.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_66.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:55:47 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 164.92 examples/s]\n",
      "01/12/2024 15:55:51 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 16.60it/s]\n",
      "01/12/2024 15:55:51 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_67.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 161.61 examples/s]\n",
      "01/12/2024 15:55:54 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 41.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_68.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:55:55 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 164.81 examples/s]\n",
      "01/12/2024 15:55:58 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 17.35it/s]\n",
      "01/12/2024 15:55:58 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_69.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 164.85 examples/s]\n",
      "01/12/2024 15:56:01 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 39.26it/s]\n",
      "01/12/2024 15:56:02 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_70.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 162.81 examples/s]\n",
      "01/12/2024 15:56:05 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 39.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_71.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:56:06 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 153.58 examples/s]\n",
      "01/12/2024 15:56:09 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 14.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_72.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:56:10 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 158.89 examples/s]\n",
      "01/12/2024 15:56:14 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 23.63it/s]\n",
      "01/12/2024 15:56:14 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_73.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 161.03 examples/s]\n",
      "01/12/2024 15:56:18 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 38.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_74.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:56:18 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 145.94 examples/s]\n",
      "01/12/2024 15:56:22 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_75.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:56:22 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 156.15 examples/s]\n",
      "01/12/2024 15:56:26 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 33.74it/s]\n",
      "01/12/2024 15:56:26 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_76.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 136.35 examples/s]\n",
      "01/12/2024 15:56:29 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 41.15it/s]\n",
      "01/12/2024 15:56:29 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_77.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 139.14 examples/s]\n",
      "01/12/2024 15:56:33 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 26.03it/s]\n",
      "01/12/2024 15:56:33 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_78.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 145.64 examples/s]\n",
      "01/12/2024 15:56:36 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 40.73it/s]\n",
      "01/12/2024 15:56:36 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_79.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 161.46 examples/s]\n",
      "01/12/2024 15:56:39 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 41.10it/s]\n",
      "01/12/2024 15:56:40 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_80.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 164.70 examples/s]\n",
      "01/12/2024 15:56:43 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 41.47it/s]\n",
      "01/12/2024 15:56:43 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_81.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 164.63 examples/s]\n",
      "01/12/2024 15:56:46 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 41.86it/s]\n",
      "01/12/2024 15:56:46 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_82.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 164.68 examples/s]\n",
      "01/12/2024 15:56:49 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 41.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_83.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:56:50 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 166.59 examples/s]\n",
      "01/12/2024 15:56:53 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 42.12it/s]\n",
      "01/12/2024 15:56:53 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_84.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 160.60 examples/s]\n",
      "01/12/2024 15:56:56 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 41.93it/s]\n",
      "01/12/2024 15:56:56 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_85.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 165.25 examples/s]\n",
      "01/12/2024 15:56:59 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 41.64it/s]\n",
      "01/12/2024 15:56:59 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_86.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 164.86 examples/s]\n",
      "01/12/2024 15:57:03 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 41.08it/s]\n",
      "01/12/2024 15:57:03 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_87.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 144.52 examples/s]\n",
      "01/12/2024 15:57:06 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 40.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_88.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "01/12/2024 15:57:06 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 161.77 examples/s]\n",
      "01/12/2024 15:57:09 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 41.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_89.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:57:09 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 155.99 examples/s]\n",
      "01/12/2024 15:57:13 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 41.16it/s]\n",
      "01/12/2024 15:57:13 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_90.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 163.82 examples/s]\n",
      "01/12/2024 15:57:16 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 41.64it/s]\n",
      "01/12/2024 15:57:16 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_91.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 163.83 examples/s]\n",
      "01/12/2024 15:57:19 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 41.73it/s]\n",
      "01/12/2024 15:57:20 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_92.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 162.68 examples/s]\n",
      "01/12/2024 15:57:23 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 39.80it/s]\n",
      "01/12/2024 15:57:23 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_93.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 161.37 examples/s]\n",
      "01/12/2024 15:57:26 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 41.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_94.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:57:26 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 162.71 examples/s]\n",
      "01/12/2024 15:57:29 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 29.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_95.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:57:30 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 160.50 examples/s]\n",
      "01/12/2024 15:57:33 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 36.64it/s]\n",
      "01/12/2024 15:57:34 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_96.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 156.17 examples/s]\n",
      "01/12/2024 15:57:37 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 43.49it/s]\n",
      "01/12/2024 15:57:37 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_97.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 161.77 examples/s]\n",
      "01/12/2024 15:57:41 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 38.88it/s]\n",
      "01/12/2024 15:57:41 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_98.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 130.55 examples/s]\n",
      "01/12/2024 15:57:44 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 38.35it/s]\n",
      "01/12/2024 15:57:44 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_99.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 138.08 examples/s]\n",
      "01/12/2024 15:57:47 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 35.24it/s]\n",
      "01/12/2024 15:57:47 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_100.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 145.91 examples/s]\n",
      "01/12/2024 15:57:52 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 28.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_101.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:57:52 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 140.04 examples/s]\n",
      "01/12/2024 15:57:56 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 38.37it/s]\n",
      "01/12/2024 15:57:56 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_102.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 158.01 examples/s]\n",
      "01/12/2024 15:57:59 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 39.72it/s]\n",
      "01/12/2024 15:57:59 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_103.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 163.06 examples/s]\n",
      "01/12/2024 15:58:02 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 39.66it/s]\n",
      "01/12/2024 15:58:02 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_104.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 162.09 examples/s]\n",
      "01/12/2024 15:58:05 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 39.59it/s]\n",
      "01/12/2024 15:58:06 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_105.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 165.18 examples/s]\n",
      "01/12/2024 15:58:08 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 38.93it/s]\n",
      "01/12/2024 15:58:09 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_106.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 161.05 examples/s]\n",
      "01/12/2024 15:58:12 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 39.62it/s]\n",
      "01/12/2024 15:58:12 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_107.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 159.88 examples/s]\n",
      "01/12/2024 15:58:15 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 38.25it/s]\n",
      "01/12/2024 15:58:15 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_108.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 174.94 examples/s]\n",
      "01/12/2024 15:58:18 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 40.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_109.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:58:18 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 159.53 examples/s]\n",
      "01/12/2024 15:58:21 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 20.12it/s]\n",
      "01/12/2024 15:58:22 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_110.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 159.97 examples/s]\n",
      "01/12/2024 15:58:25 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 30.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_111.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:58:26 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 99.63 examples/s]\n",
      "01/12/2024 15:58:29 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.69it/s]\n",
      "01/12/2024 15:58:29 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_112.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 180.91 examples/s]\n",
      "01/12/2024 15:58:33 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 41.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_113.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:58:36 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 139.93 examples/s]\n",
      "01/12/2024 15:58:40 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.80it/s]\n",
      "01/12/2024 15:58:40 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_114.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 185.65 examples/s]\n",
      "01/12/2024 15:58:43 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 43.81it/s]\n",
      "01/12/2024 15:58:43 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_115.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 183.37 examples/s]\n",
      "01/12/2024 15:58:46 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 43.66it/s]\n",
      "01/12/2024 15:58:46 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_116.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 166.15 examples/s]\n",
      "01/12/2024 15:58:49 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 28.79it/s]\n",
      "01/12/2024 15:58:50 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_117.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 160.97 examples/s]\n",
      "01/12/2024 15:58:53 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 42.36it/s]\n",
      "01/12/2024 15:58:53 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_118.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 160.58 examples/s]\n",
      "01/12/2024 15:58:56 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 42.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_119.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:58:56 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 160.43 examples/s]\n",
      "01/12/2024 15:59:00 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 40.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_120.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:59:00 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 163.22 examples/s]\n",
      "01/12/2024 15:59:03 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 34.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_121.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:59:03 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 162.43 examples/s]\n",
      "01/12/2024 15:59:07 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 34.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_122.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:59:07 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 154.99 examples/s]\n",
      "01/12/2024 15:59:10 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 39.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_123.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:59:10 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 161.99 examples/s]\n",
      "01/12/2024 15:59:14 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 41.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_124.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:59:14 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 142.93 examples/s]\n",
      "01/12/2024 15:59:17 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 35.48it/s]\n",
      "01/12/2024 15:59:17 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_125.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 139.67 examples/s]\n",
      "01/12/2024 15:59:21 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 29.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_126.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:59:22 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 159.27 examples/s]\n",
      "01/12/2024 15:59:25 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 37.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  67379_127.coref\n",
      "ERROR:  6532\n",
      "[Errno 2] No such file or directory: '../meetups_pilot/indexedParagraphs/6532.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:59:26 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 148.11 examples/s]\n",
      "01/12/2024 15:59:29 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 14.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_0.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:59:30 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 139.35 examples/s]\n",
      "01/12/2024 15:59:34 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 17.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_1.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:59:34 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 148.42 examples/s]\n",
      "01/12/2024 15:59:38 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 15.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_2.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:59:39 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 158.81 examples/s]\n",
      "01/12/2024 15:59:42 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.85it/s]\n",
      "01/12/2024 15:59:43 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_3.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 162.27 examples/s]\n",
      "01/12/2024 15:59:46 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 42.14it/s]\n",
      "01/12/2024 15:59:46 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_4.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 159.16 examples/s]\n",
      "01/12/2024 15:59:49 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 41.44it/s]\n",
      "01/12/2024 15:59:49 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_5.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 164.55 examples/s]\n",
      "01/12/2024 15:59:52 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 38.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_6.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 15:59:53 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 143.48 examples/s]\n",
      "01/12/2024 15:59:57 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.24it/s]\n",
      "01/12/2024 15:59:57 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_7.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 162.69 examples/s]\n",
      "01/12/2024 16:00:00 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 37.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_8.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 16:00:00 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 147.51 examples/s]\n",
      "01/12/2024 16:00:03 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_9.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 16:00:05 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 127.03 examples/s]\n",
      "01/12/2024 16:00:08 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_10.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 16:00:09 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 165.14 examples/s]\n",
      "01/12/2024 16:00:12 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 38.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_11.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 16:00:13 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 126.47 examples/s]\n",
      "01/12/2024 16:00:16 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.50it/s]\n",
      "01/12/2024 16:00:17 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_12.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 163.52 examples/s]\n",
      "01/12/2024 16:00:20 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 38.71it/s]\n",
      "01/12/2024 16:00:20 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_13.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 166.73 examples/s]\n",
      "01/12/2024 16:00:23 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 33.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_14.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 16:00:24 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 110.10 examples/s]\n",
      "01/12/2024 16:00:28 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.12it/s]\n",
      "01/12/2024 16:00:28 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_15.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 153.35 examples/s]\n",
      "01/12/2024 16:00:32 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 40.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_16.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 16:00:33 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 114.27 examples/s]\n",
      "01/12/2024 16:00:37 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_17.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 16:00:38 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 181.04 examples/s]\n",
      "01/12/2024 16:00:41 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 40.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_18.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 16:00:42 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 103.88 examples/s]\n",
      "01/12/2024 16:00:46 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.79it/s]\n",
      "01/12/2024 16:00:47 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_19.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 185.34 examples/s]\n",
      "01/12/2024 16:00:50 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 43.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_20.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 16:00:50 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 141.17 examples/s]\n",
      "01/12/2024 16:00:54 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_21.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 16:00:56 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 110.21 examples/s]\n",
      "01/12/2024 16:00:59 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.88it/s]\n",
      "01/12/2024 16:01:00 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_22.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 163.20 examples/s]\n",
      "01/12/2024 16:01:03 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 41.08it/s]\n",
      "01/12/2024 16:01:03 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_23.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 164.66 examples/s]\n",
      "01/12/2024 16:01:06 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 25.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_24.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 16:01:07 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 120.88 examples/s]\n",
      "01/12/2024 16:01:11 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_25.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 16:01:12 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 148.32 examples/s]\n",
      "01/12/2024 16:01:15 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 14.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_26.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 16:01:16 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 129.11 examples/s]\n",
      "01/12/2024 16:01:20 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 14.83it/s]\n",
      "01/12/2024 16:01:20 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_27.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 152.99 examples/s]\n",
      "01/12/2024 16:01:23 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 41.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_28.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 16:01:24 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 120.52 examples/s]\n",
      "01/12/2024 16:01:28 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.91it/s]\n",
      "01/12/2024 16:01:28 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_29.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 165.08 examples/s]\n",
      "01/12/2024 16:01:31 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 39.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_30.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 16:01:32 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 161.53 examples/s]\n",
      "01/12/2024 16:01:35 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 31.69it/s]\n",
      "01/12/2024 16:01:35 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_31.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 161.89 examples/s]\n",
      "01/12/2024 16:01:38 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 37.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_32.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 16:01:38 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 110.62 examples/s]\n",
      "01/12/2024 16:01:41 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.76it/s]\n",
      "01/12/2024 16:01:41 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_33.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 160.74 examples/s]\n",
      "01/12/2024 16:01:44 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 38.16it/s]\n",
      "01/12/2024 16:01:44 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_34.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 138.72 examples/s]\n",
      "01/12/2024 16:01:47 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 37.09it/s]\n",
      "01/12/2024 16:01:48 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_35.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 157.95 examples/s]\n",
      "01/12/2024 16:01:50 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 39.47it/s]\n",
      "01/12/2024 16:01:51 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_36.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 160.73 examples/s]\n",
      "01/12/2024 16:01:54 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 39.75it/s]\n",
      "01/12/2024 16:01:54 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_37.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 160.30 examples/s]\n",
      "01/12/2024 16:01:57 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 39.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_38.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 16:01:57 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 143.13 examples/s]\n",
      "01/12/2024 16:02:00 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 40.74it/s]\n",
      "01/12/2024 16:02:00 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_39.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 161.30 examples/s]\n",
      "01/12/2024 16:02:03 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 39.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_40.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 16:02:05 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 103.91 examples/s]\n",
      "01/12/2024 16:02:09 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.26it/s]\n",
      "01/12/2024 16:02:09 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_41.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 154.56 examples/s]\n",
      "01/12/2024 16:02:12 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 39.55it/s]\n",
      "01/12/2024 16:02:12 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_42.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 164.90 examples/s]\n",
      "01/12/2024 16:02:15 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 31.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_43.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 16:02:15 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 150.54 examples/s]\n",
      "01/12/2024 16:02:19 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 26.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_44.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 16:02:19 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 153.93 examples/s]\n",
      "01/12/2024 16:02:22 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 27.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_45.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 16:02:22 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 145.22 examples/s]\n",
      "01/12/2024 16:02:26 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 26.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_46.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 16:02:27 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 126.95 examples/s]\n",
      "01/12/2024 16:02:30 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.29it/s]\n",
      "01/12/2024 16:02:30 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_47.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 181.45 examples/s]\n",
      "01/12/2024 16:02:33 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 41.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_48.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 16:02:34 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 129.73 examples/s]\n",
      "01/12/2024 16:02:38 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_49.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 16:02:39 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 127.68 examples/s]\n",
      "01/12/2024 16:02:43 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_50.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 16:02:43 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 155.73 examples/s]\n",
      "01/12/2024 16:02:47 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 24.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_51.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 16:02:48 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 119.77 examples/s]\n",
      "01/12/2024 16:02:51 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.68it/s]\n",
      "01/12/2024 16:02:51 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_52.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 146.43 examples/s]\n",
      "01/12/2024 16:02:54 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 41.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_53.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 16:02:55 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 161.23 examples/s]\n",
      "01/12/2024 16:02:57 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 19.26it/s]\n",
      "01/12/2024 16:02:58 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_54.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 182.61 examples/s]\n",
      "01/12/2024 16:03:00 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 40.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_55.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 16:03:02 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 77.45 examples/s]\n",
      "01/12/2024 16:03:06 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_56.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 16:03:07 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 155.46 examples/s]\n",
      "01/12/2024 16:03:10 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 17.43it/s]\n",
      "01/12/2024 16:03:10 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_57.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 173.88 examples/s]\n",
      "01/12/2024 16:03:13 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 41.61it/s]\n",
      "01/12/2024 16:03:13 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_58.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 180.96 examples/s]\n",
      "01/12/2024 16:03:17 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 41.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_59.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 16:03:18 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 114.30 examples/s]\n",
      "01/12/2024 16:03:21 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.10it/s]\n",
      "01/12/2024 16:03:22 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_60.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 161.52 examples/s]\n",
      "01/12/2024 16:03:25 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 40.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_61.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 16:03:26 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 134.28 examples/s]\n",
      "01/12/2024 16:03:30 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.02it/s]\n",
      "01/12/2024 16:03:30 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_62.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 159.73 examples/s]\n",
      "01/12/2024 16:03:33 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 36.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_63.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 16:03:34 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 129.83 examples/s]\n",
      "01/12/2024 16:03:37 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 21.26it/s]\n",
      "01/12/2024 16:03:37 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_64.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 164.37 examples/s]\n",
      "01/12/2024 16:03:40 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 41.93it/s]\n",
      "01/12/2024 16:03:40 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_65.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 164.56 examples/s]\n",
      "01/12/2024 16:03:43 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 41.89it/s]\n",
      "01/12/2024 16:03:43 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_66.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 161.20 examples/s]\n",
      "01/12/2024 16:03:46 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 40.56it/s]\n",
      "01/12/2024 16:03:46 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_67.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 146.98 examples/s]\n",
      "01/12/2024 16:03:49 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 39.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_68.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 16:03:50 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 158.65 examples/s]\n",
      "01/12/2024 16:03:53 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 26.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_69.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 16:03:54 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 162.34 examples/s]\n",
      "01/12/2024 16:03:57 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 26.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_70.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 16:03:57 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 156.87 examples/s]\n",
      "01/12/2024 16:04:00 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 25.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_71.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 16:04:01 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 162.09 examples/s]\n",
      "01/12/2024 16:04:04 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 32.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_72.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 16:04:04 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 157.50 examples/s]\n",
      "01/12/2024 16:04:08 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 26.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_73.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 16:04:09 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 162.01 examples/s]\n",
      "01/12/2024 16:04:12 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 30.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_74.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 16:04:12 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 150.61 examples/s]\n",
      "01/12/2024 16:04:15 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 15.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_75.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 16:04:16 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 137.13 examples/s]\n",
      "01/12/2024 16:04:20 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 25.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_76.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 16:04:20 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 157.86 examples/s]\n",
      "01/12/2024 16:04:23 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 21.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_77.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 16:04:24 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 150.15 examples/s]\n",
      "01/12/2024 16:04:28 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 22.11it/s]\n",
      "01/12/2024 16:04:28 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_78.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 162.22 examples/s]\n",
      "01/12/2024 16:04:31 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 41.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_79.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 16:04:31 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 139.92 examples/s]\n",
      "01/12/2024 16:04:35 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 23.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_80.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 16:04:35 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 163.67 examples/s]\n",
      "01/12/2024 16:04:38 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 35.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_81.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 16:04:38 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 157.40 examples/s]\n",
      "01/12/2024 16:04:42 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 33.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_82.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 16:04:42 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 161.34 examples/s]\n",
      "01/12/2024 16:04:45 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 40.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_83.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 16:04:45 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 164.88 examples/s]\n",
      "01/12/2024 16:04:48 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 36.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_84.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 16:04:49 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 151.08 examples/s]\n",
      "01/12/2024 16:04:52 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 36.20it/s]\n",
      "01/12/2024 16:04:52 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_85.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 155.44 examples/s]\n",
      "01/12/2024 16:04:55 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 38.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_86.coref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 16:04:56 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 161.49 examples/s]\n",
      "01/12/2024 16:04:59 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 30.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc to disk:  45181_87.coref\n",
      "ERROR:  2363\n",
      "[Errno 2] No such file or directory: '../meetups_pilot/indexedParagraphs/2363.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# read each paragraph in the biography\n",
    "for bio in no_found:\n",
    "    try:\n",
    "        coref_text = []\n",
    "        df_prg = pd.read_csv('../meetups_pilot/indexedParagraphs/'+str(bio)+'.csv')\n",
    "        for prg in df_prg.itertuples():\n",
    "            doc = nlp(prg.paragraph, component_cfg={\"fastcoref\":{\"resolve_text\":True}})\n",
    "            coref_text.append([prg.paragraph,prg.paragraphIndex,doc._.resolved_text])\n",
    "            print('doc to disk: ', bio+'_'+str(prg.paragraphIndex)+'.coref')\n",
    "            doc.to_disk(\"doc-objects/\"+bio+'_'+str(prg.paragraphIndex)+'.coref')\n",
    "    \n",
    "         # save results\n",
    "        if len(coref_text) > 0:\n",
    "            # Create a DataFrame from the list of lists\n",
    "            df_result = pd.DataFrame(coref_text, columns=['ori_text', 'paragraphIndex','coref_text'])\n",
    "            # Save the DataFrame to a CSV file\n",
    "            df_result.to_csv('results-coref-text/'+str(bio)+'.csv', index=False)\n",
    "    except Exception as e: \n",
    "        print(\"ERROR: \", bio)\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f71a7ad-3845-497f-9ef4-a99806640b5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
